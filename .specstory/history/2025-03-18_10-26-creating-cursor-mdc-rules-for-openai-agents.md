<!-- Generated by SpecStory -->

# Creating Cursor .mdc Rules for OpenAI Agents (2025-03-18 12:26:38)

_**User**_

this is a clone of an example folder on how to use agents using the new agents SDK from openAI. 

I want you to learn the documentation and examples and distil them into severa cursor .mdc rules to be used in the future

---

_**Assistant**_

I'll help you create Cursor .mdc rules based on the OpenAI agents SDK documentation and examples. Let's start by exploring the codebase to understand the structure and key concepts.

---

<details>
            <summary>Listed current directory â€¢ **17** results</summary>
        
| Name |
|-------|
| ğŸ“ `.cursor` |
| ğŸ“ `env` |
| ğŸ“ `.specstory` |
| ğŸ“ `.git` |
| ğŸ“„ `uv.lock` |
| ğŸ“ `tests` |
| ğŸ“ `src` |
| ğŸ“„ `mkdocs.yml` |
| ğŸ“„ `pyproject.toml` |
| ğŸ“ `examples` |
| ğŸ“ `docs` |
| ğŸ“„ `README.md` |
| ğŸ“„ `.gitignore` |
| ğŸ“„ `.prettierrc` |
| ğŸ“„ `LICENSE` |
| ğŸ“„ `Makefile` |
| ğŸ“ `.github` |

</details>

---

Let's first check the README to get an overview of the project.

---

Read file: README.md

---

Now let's look at the examples to understand how agents are implemented:

---

<details>
            <summary>Listed directory examples â€¢ **8** results</summary>
        
| Name |
|-------|
| ğŸ“ `tools` |
| ğŸ“ `research_bot` |
| ğŸ“ `model_providers` |
| ğŸ“ `basic` |
| ğŸ“ `customer_service` |
| ğŸ“ `handoffs` |
| ğŸ“ `agent_patterns` |
| ğŸ“„ `__init__.py` |

</details>

---

Let's look at one of the basic examples to understand the fundamental implementation:

---

<details>
            <summary>Listed directory examples/basic â€¢ **7** results</summary>
        
| Name |
|-------|
| ğŸ“„ `stream_text.py` |
| ğŸ“„ `dynamic_system_prompt.py` |
| ğŸ“„ `hello_world.py` |
| ğŸ“„ `hello_world_jupyter.py` |
| ğŸ“„ `lifecycle_example.py` |
| ğŸ“„ `stream_items.py` |
| ğŸ“„ `agent_lifecycle_example.py` |

</details>

---

Let's look at the hello_world.py example:

---

Read file: examples/basic/hello_world.py

---

Now, let's examine an example with tools:

---

<details>
            <summary>Listed directory examples/tools â€¢ **3** results</summary>
        
| Name |
|-------|
| ğŸ“„ `web_search.py` |
| ğŸ“„ `computer_use.py` |
| ğŸ“„ `file_search.py` |

</details>

---

Read file: examples/tools/web_search.py

---

Let's look at the handoffs examples:

---

<details>
            <summary>Listed directory examples/handoffs â€¢ **2** results</summary>
        
| Name |
|-------|
| ğŸ“„ `message_filter_streaming.py` |
| ğŸ“„ `message_filter.py` |

</details>

---

Read file: examples/handoffs/message_filter.py

---

Now let's check the agent patterns to understand more complex implementations:

---

<details>
            <summary>Listed directory examples/agent_patterns â€¢ **8** results</summary>
        
| Name |
|-------|
| ğŸ“„ `output_guardrails.py` |
| ğŸ“„ `parallelization.py` |
| ğŸ“„ `routing.py` |
| ğŸ“„ `deterministic.py` |
| ğŸ“„ `input_guardrails.py` |
| ğŸ“„ `llm_as_a_judge.py` |
| ğŸ“„ `README.md` |
| ğŸ“„ `agents_as_tools.py` |

</details>

---

Let's take a look at the agent patterns README:

---

Read file: examples/agent_patterns/README.md

---

Let's check one of these patterns to better understand the implementation:

---

Read file: examples/agent_patterns/deterministic.py

---

Now, let's learn about guardrails:

---

Read file: examples/agent_patterns/input_guardrails.py

---

Let's also look at one more example to understand tracing:

---

Read file: examples/basic/agent_lifecycle_example.py

---

Now that we have a good understanding of the OpenAI agents SDK, let's create Cursor .mdc rules to document key concepts and usage patterns.

Let's create several rules for different aspects of the OpenAI Agents SDK:

---



---

I'll need to create these rules locally instead since we can't access the GitHub repository directly.

---

```diff
- 
+ # OpenAI Agents SDK - Basic Usage
+ 
+ ## Creating and Running Agents
+ 
+ Basic structure for creating and running agents:
+ 
+ ```python
+ from agents import Agent, Runner
+ import asyncio
+ 
+ # Create an agent with instructions
+ agent = Agent(
+     name="Assistant",
+     instructions="You are a helpful assistant."
+ )
+ 
+ # Run the agent synchronously
+ async def main():
+     result = await Runner.run(agent, "Your query here")
+     print(result.final_output)
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ```
+ 
+ ## Key Components
+ 
+ - **Agent**: Defines behavior with instructions, name, and optional tools/handoffs
+ - **Runner**: Manages the execution of agents
+ - **run/run_sync**: Methods to execute agents (async/sync)
+ 
+ ## Basic Agent Parameters
+ 
+ - `name`: Identifier for the agent
+ - `instructions`: Behavior guidance for the agent 
+ - `tools`: List of tools the agent can use
+ - `handoffs`: List of agents this agent can transfer control to
+ - `model`: Model to use (default is GPT-4)
+ - `output_type`: Expected return type for structured outputs
+ 
+ ## Agent Execution Loop
+ 
+ 1. Agent receives input
+ 2. LLM generates response (may include tool calls)
+ 3. If response has final output, loop ends
+ 4. If response has handoff, control transfers to new agent
+ 5. If response has tool calls, tools execute and loop continues
+ 
+ ## Example: Hello World
+ 
+ ```python
+ import asyncio
+ from agents import Agent, Runner
+ 
+ async def main():
+     agent = Agent(
+         name="Assistant",
+         instructions="You only respond in haikus."
+     )
+     
+     result = await Runner.run(agent, "Tell me about recursion in programming.")
+     print(result.final_output)
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ```
+ 
+ ## Environment Setup
+ 
+ Set `OPENAI_API_KEY` environment variable before running agents. 
```

---

Let's create the next rule for tools:

---

```diff
- 
+ # OpenAI Agents SDK - Tools
+ 
+ ## Function Tools
+ 
+ Create function tools using the `@function_tool` decorator:
+ 
+ ```python
+ from agents import Agent, Runner, function_tool
+ import asyncio
+ 
+ @function_tool
+ def get_weather(city: str) -> str:
+     """Return weather for a given city."""
+     return f"The weather in {city} is sunny."
+ 
+ async def main():
+     agent = Agent(
+         name="Weather Assistant",
+         instructions="You help users check the weather.",
+         tools=[get_weather]
+     )
+     
+     result = await Runner.run(agent, "What's the weather in Tokyo?")
+     print(result.final_output)
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ```
+ 
+ ## Built-in Tools
+ 
+ The SDK provides several built-in tools:
+ 
+ ```python
+ from agents import WebSearchTool, Agent, Runner
+ 
+ agent = Agent(
+     name="Web searcher",
+     instructions="You search the web for information.",
+     tools=[WebSearchTool(user_location={"type": "approximate", "city": "New York"})]
+ )
+ ```
+ 
+ ## Tool Parameters
+ 
+ - Tool functions should have type annotations for parameters and return values
+ - Include docstrings to help the agent understand tool purpose
+ - Tools are passed to Agent as a list: `tools=[tool1, tool2]`
+ 
+ ## Advanced Tool Usage
+ 
+ ### Custom Tool Class
+ 
+ For complex tools, create a custom class:
+ 
+ ```python
+ from agents import Tool
+ from typing import Any, Dict
+ 
+ class CustomTool(Tool):
+     name = "custom_tool"
+     description = "Performs a custom operation"
+     
+     async def invoke(self, parameters: Dict[str, Any]) -> str:
+         # Custom implementation
+         return "Result of custom operation"
+ ```
+ 
+ ### Tool Events
+ 
+ Track tool execution with hooks:
+ 
+ ```python
+ from agents import Agent, AgentHooks, RunContextWrapper, Tool
+ 
+ class CustomHooks(AgentHooks):
+     async def on_tool_start(self, context: RunContextWrapper, agent: Agent, tool: Tool) -> None:
+         print(f"Agent {agent.name} started tool {tool.name}")
+         
+     async def on_tool_end(self, context: RunContextWrapper, agent: Agent, tool: Tool, result: str) -> None:
+         print(f"Agent {agent.name} finished tool {tool.name} with result: {result}")
+ 
+ agent = Agent(
+     name="Tool User",
+     instructions="Use tools to accomplish tasks.",
+     tools=[my_tool],
+     hooks=CustomHooks()
+ )
+ ```
+ 
+ ## Example: Multiple Tools
+ 
+ ```python
+ from agents import Agent, Runner, function_tool
+ import asyncio
+ 
+ @function_tool
+ def add_numbers(a: int, b: int) -> int:
+     """Add two numbers together."""
+     return a + b
+ 
+ @function_tool
+ def multiply_numbers(a: int, b: int) -> int:
+     """Multiply two numbers together."""
+     return a * b
+ 
+ async def main():
+     agent = Agent(
+         name="Math Assistant",
+         instructions="You help with math calculations.",
+         tools=[add_numbers, multiply_numbers]
+     )
+     
+     result = await Runner.run(agent, "I need to add 5 and 7, then multiply by 2.")
+     print(result.final_output)
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ``` 
```

---

Now let's create a rule for handoffs:

---

```diff
- 
+ # OpenAI Agents SDK - Handoffs
+ 
+ ## Basic Handoff
+ 
+ Handoffs allow agents to transfer control to other agents:
+ 
+ ```python
+ from agents import Agent, Runner
+ import asyncio
+ 
+ spanish_agent = Agent(
+     name="Spanish Agent",
+     instructions="You only speak Spanish."
+ )
+ 
+ english_agent = Agent(
+     name="English Agent",
+     instructions="You only speak English."
+ )
+ 
+ triage_agent = Agent(
+     name="Triage Agent",
+     instructions="Handoff to the appropriate agent based on the language of the request.",
+     handoffs=[spanish_agent, english_agent]
+ )
+ 
+ async def main():
+     result = await Runner.run(triage_agent, "Hola, Â¿cÃ³mo estÃ¡s?")
+     print(result.final_output)
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ```
+ 
+ ## Handoff Parameters
+ 
+ - Configure handoffs by providing a list of agents: `handoffs=[agent1, agent2]`
+ - Agents must be initialized before being passed to handoffs
+ - The handoffs list defines which agents can receive control
+ 
+ ## Handoff Control Flow
+ 
+ 1. The current agent decides to handoff to another agent
+ 2. It makes a handoff "tool call" specifying the target agent
+ 3. The target agent receives control and continues execution
+ 4. The handoff target receives the conversation history
+ 
+ ## Customizing Handoffs with Filters
+ 
+ Use input filters to modify what the target agent receives:
+ 
+ ```python
+ from agents import Agent, HandoffInputData, Runner, handoff, handoff_filters
+ 
+ def custom_filter(handoff_data: HandoffInputData) -> HandoffInputData:
+     # Remove tool-related messages from history
+     filtered_data = handoff_filters.remove_all_tools(handoff_data)
+     
+     # Customize what the target agent sees
+     return filtered_data
+ 
+ agent_a = Agent(
+     name="Agent A",
+     instructions="You are the first agent."
+ )
+ 
+ agent_b = Agent(
+     name="Agent B",
+     instructions="You are the second agent."
+ )
+ 
+ main_agent = Agent(
+     name="Main Agent",
+     instructions="Coordinate between agents.",
+     handoffs=[
+         handoff(agent_a),  # Standard handoff
+         handoff(agent_b, input_filter=custom_filter)  # With custom filter
+     ]
+ )
+ ```
+ 
+ ## Handoff Events
+ 
+ Track handoffs with hooks:
+ 
+ ```python
+ from agents import Agent, AgentHooks, RunContextWrapper
+ 
+ class CustomHooks(AgentHooks):
+     async def on_handoff(self, context: RunContextWrapper, agent: Agent, source: Agent) -> None:
+         print(f"Handoff from {source.name} to {agent.name}")
+ 
+ agent = Agent(
+     name="Handoff User",
+     hooks=CustomHooks(),
+     handoffs=[other_agent]
+ )
+ ```
+ 
+ ## Routing Pattern Example
+ 
+ ```python
+ from agents import Agent, Runner
+ import asyncio
+ 
+ # Specialized agents
+ math_agent = Agent(
+     name="Math Agent",
+     instructions="You solve math problems only."
+ )
+ 
+ weather_agent = Agent(
+     name="Weather Agent",
+     instructions="You provide weather information only."
+ )
+ 
+ trivia_agent = Agent(
+     name="Trivia Agent",
+     instructions="You answer trivia questions only."
+ )
+ 
+ # Router agent
+ router_agent = Agent(
+     name="Router",
+     instructions="""
+     You're a routing agent that determines which specialized agent should handle a request.
+     - Math Agent: For math calculations and problems
+     - Weather Agent: For weather-related questions
+     - Trivia Agent: For general knowledge questions
+     
+     Make a handoff to the appropriate agent based on the user's query.
+     """,
+     handoffs=[math_agent, weather_agent, trivia_agent]
+ )
+ 
+ async def main():
+     while True:
+         user_query = input("Ask a question (or type 'exit' to quit): ")
+         if user_query.lower() == 'exit':
+             break
+             
+         result = await Runner.run(router_agent, user_query)
+         print(f"Answer: {result.final_output}")
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ``` 
```

---

Let's create a rule for guardrails:

---

```diff
- 
+ # OpenAI Agents SDK - Guardrails
+ 
+ ## Input Guardrails
+ 
+ Input guardrails let you validate and filter user inputs before they reach your agent:
+ 
+ ```python
+ from agents import Agent, GuardrailFunctionOutput, InputGuardrailTripwireTriggered, RunContextWrapper, Runner, input_guardrail
+ from pydantic import BaseModel
+ 
+ class ContentCheckOutput(BaseModel):
+     reasoning: str
+     is_inappropriate: bool
+ 
+ guardrail_agent = Agent(
+     name="Guardrail Check",
+     instructions="Check if the input contains inappropriate content.",
+     output_type=ContentCheckOutput
+ )
+ 
+ @input_guardrail
+ async def content_guardrail(context, agent, input):
+     result = await Runner.run(guardrail_agent, input, context=context.context)
+     final_output = result.final_output_as(ContentCheckOutput)
+     
+     return GuardrailFunctionOutput(
+         output_info=final_output,
+         tripwire_triggered=final_output.is_inappropriate
+     )
+ 
+ main_agent = Agent(
+     name="Main Agent",
+     instructions="You are a helpful assistant.",
+     input_guardrails=[content_guardrail]
+ )
+ 
+ try:
+     result = await Runner.run(main_agent, user_input)
+     print(result.final_output)
+ except InputGuardrailTripwireTriggered:
+     print("Sorry, I cannot process that request.")
+ ```
+ 
+ ## Output Guardrails
+ 
+ Output guardrails validate your agent's responses before they're returned:
+ 
+ ```python
+ from agents import Agent, GuardrailFunctionOutput, OutputGuardrailTripwireTriggered, RunContextWrapper, Runner, output_guardrail
+ 
+ @output_guardrail
+ async def tone_guardrail(context, agent, output, input):
+     # Check if output maintains the appropriate tone
+     if "rude phrase" in output.lower():
+         return GuardrailFunctionOutput(
+             output_info={"reason": "Response contains rude language"},
+             tripwire_triggered=True
+         )
+     return GuardrailFunctionOutput(tripwire_triggered=False)
+ 
+ agent = Agent(
+     name="Polite Assistant",
+     instructions="You are a very polite assistant.",
+     output_guardrails=[tone_guardrail]
+ )
+ 
+ try:
+     result = await Runner.run(agent, user_input)
+     print(result.final_output)
+ except OutputGuardrailTripwireTriggered:
+     print("The agent's response did not meet our standards.")
+ ```
+ 
+ ## Guardrail Parameters
+ 
+ - Input guardrails use the `@input_guardrail` decorator
+ - Output guardrails use the `@output_guardrail` decorator
+ - Both return a `GuardrailFunctionOutput` with:
+   - `tripwire_triggered`: Boolean indicating if the guardrail was triggered
+   - `output_info`: Optional data providing context for the guardrail decision
+ 
+ ## Complete Guardrail Example
+ 
+ ```python
+ from agents import (
+     Agent, 
+     GuardrailFunctionOutput, 
+     InputGuardrailTripwireTriggered,
+     OutputGuardrailTripwireTriggered,
+     RunContextWrapper, 
+     Runner, 
+     input_guardrail,
+     output_guardrail
+ )
+ from pydantic import BaseModel
+ import asyncio
+ 
+ # Input guardrail
+ @input_guardrail
+ async def profanity_guardrail(context, agent, input):
+     # Check for inappropriate content
+     contains_profanity = any(word in str(input).lower() for word in ["bad_word1", "bad_word2"])
+     
+     return GuardrailFunctionOutput(
+         output_info={"reason": "Contains profanity" if contains_profanity else "Clean"},
+         tripwire_triggered=contains_profanity
+     )
+ 
+ # Output guardrail
+ @output_guardrail
+ async def output_length_guardrail(context, agent, output, input):
+     # Ensure output isn't too long
+     if len(output) > 500:
+         return GuardrailFunctionOutput(
+             output_info={"reason": "Response too long"},
+             tripwire_triggered=True
+         )
+     return GuardrailFunctionOutput(tripwire_triggered=False)
+ 
+ # Main agent with both guardrails
+ agent = Agent(
+     name="Safe Assistant",
+     instructions="You are a helpful and concise assistant.",
+     input_guardrails=[profanity_guardrail],
+     output_guardrails=[output_length_guardrail]
+ )
+ 
+ async def main():
+     user_input = input("Ask a question: ")
+     
+     try:
+         result = await Runner.run(agent, user_input)
+         print(f"Response: {result.final_output}")
+     except InputGuardrailTripwireTriggered as e:
+         print(f"Input rejected: {e}")
+     except OutputGuardrailTripwireTriggered as e:
+         print(f"Output rejected: {e}")
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ```
+ 
+ ## Best Practices
+ 
+ 1. **Fail Fast**: Use guardrails to catch issues early in your agent workflows
+ 2. **Defense in Depth**: Use both input and output guardrails for critical applications
+ 3. **Provide Context**: Include reasoning in `output_info` for better debugging
+ 4. **Performance**: Keep guardrail checks light and fast if possible
+ 5. **Error Handling**: Always wrap agent runs in try/except to handle tripwires 
```

---

Let's create a rule for common agent patterns:

---

```diff
- 
+ # OpenAI Agents SDK - Common Agent Patterns
+ 
+ ## Deterministic Flows
+ 
+ Break tasks into a series of steps with specialized agents:
+ 
+ ```python
+ from agents import Agent, Runner
+ from pydantic import BaseModel
+ import asyncio
+ 
+ # Step 1: Generate outline
+ outline_agent = Agent(
+     name="Outline Generator",
+     instructions="Generate a short outline based on the user's input."
+ )
+ 
+ # Step 2: Check quality
+ class QualityCheckOutput(BaseModel):
+     good_quality: bool
+     reasoning: str
+ 
+ quality_checker_agent = Agent(
+     name="Quality Checker",
+     instructions="Check if the outline meets quality standards.",
+     output_type=QualityCheckOutput
+ )
+ 
+ # Step 3: Generate final content
+ content_agent = Agent(
+     name="Content Generator",
+     instructions="Create full content based on the approved outline."
+ )
+ 
+ async def main():
+     # Step 1: Generate outline
+     outline_result = await Runner.run(outline_agent, user_input)
+     
+     # Step 2: Check quality
+     check_result = await Runner.run(quality_checker_agent, outline_result.final_output)
+     quality_check = check_result.final_output_as(QualityCheckOutput)
+     
+     # Only proceed if quality is good
+     if not quality_check.good_quality:
+         print(f"Outline rejected: {quality_check.reasoning}")
+         return
+         
+     # Step 3: Generate final content
+     content_result = await Runner.run(content_agent, outline_result.final_output)
+     print(content_result.final_output)
+ ```
+ 
+ ## Routing Pattern
+ 
+ Create specialized agents and a router to direct traffic:
+ 
+ ```python
+ from agents import Agent, Runner
+ import asyncio
+ 
+ # Specialized agents
+ support_agent = Agent(
+     name="Support Agent",
+     instructions="Handle customer support issues."
+ )
+ 
+ sales_agent = Agent(
+     name="Sales Agent",
+     instructions="Handle sales inquiries."
+ )
+ 
+ technical_agent = Agent(
+     name="Technical Agent",
+     instructions="Handle technical questions."
+ )
+ 
+ # Router agent
+ router_agent = Agent(
+     name="Router",
+     instructions="""Route inquiries to the appropriate specialized agent:
+     - Support Agent: For customer service issues
+     - Sales Agent: For pricing, ordering, and product availability
+     - Technical Agent: For technical questions and troubleshooting
+     """,
+     handoffs=[support_agent, sales_agent, technical_agent]
+ )
+ ```
+ 
+ ## Agents as Tools
+ 
+ Use agents as tools within other agents:
+ 
+ ```python
+ from agents import Agent, Runner, function_tool
+ import asyncio
+ 
+ # Create worker agents
+ translator_agent = Agent(
+     name="Translator",
+     instructions="Translate text to the specified language."
+ )
+ 
+ summarizer_agent = Agent(
+     name="Summarizer",
+     instructions="Summarize the provided text concisely."
+ )
+ 
+ # Create tool functions that use the agents
+ @function_tool
+ async def translate_text(text: str, target_language: str) -> str:
+     """Translate text to the specified language."""
+     result = await Runner.run(
+         translator_agent,
+         f"Translate this to {target_language}: {text}"
+     )
+     return result.final_output
+ 
+ @function_tool
+ async def summarize_text(text: str, max_length: int = 100) -> str:
+     """Summarize text within the specified length."""
+     result = await Runner.run(
+         summarizer_agent,
+         f"Summarize this text in {max_length} characters or less: {text}"
+     )
+     return result.final_output
+ 
+ # Main agent that uses the tool agents
+ coordinator_agent = Agent(
+     name="Coordinator",
+     instructions="Help users process text using translation and summarization.",
+     tools=[translate_text, summarize_text]
+ )
+ ```
+ 
+ ## LLM as a Judge
+ 
+ Use one agent to evaluate another's output:
+ 
+ ```python
+ from agents import Agent, Runner
+ from pydantic import BaseModel
+ import asyncio
+ 
+ # Output creator
+ content_agent = Agent(
+     name="Content Creator",
+     instructions="Create content based on the user's request."
+ )
+ 
+ # Evaluation model
+ class ContentEvaluation(BaseModel):
+     score: int  # 1-10
+     feedback: str
+     suggestions: list[str]
+ 
+ judge_agent = Agent(
+     name="Content Judge",
+     instructions="Evaluate content on a scale of 1-10 and provide feedback.",
+     output_type=ContentEvaluation
+ )
+ 
+ async def main():
+     # Generate content
+     content_result = await Runner.run(content_agent, user_request)
+     content = content_result.final_output
+     
+     # Evaluate content
+     eval_result = await Runner.run(
+         judge_agent,
+         f"Please evaluate this content:\n\n{content}"
+     )
+     evaluation = eval_result.final_output_as(ContentEvaluation)
+     
+     # Handle based on score
+     if evaluation.score >= 7:
+         print(f"Content approved (score: {evaluation.score}/10)")
+         return content
+     else:
+         print(f"Content needs improvement (score: {evaluation.score}/10)")
+         print(f"Feedback: {evaluation.feedback}")
+         
+         # Optionally: Regenerate with feedback
+         improved_result = await Runner.run(
+             content_agent,
+             f"Original request: {user_request}\n\nFeedback: {evaluation.feedback}\n\nPlease create improved content."
+         )
+         return improved_result.final_output
+ ```
+ 
+ ## Parallelization
+ 
+ Run multiple agents in parallel:
+ 
+ ```python
+ from agents import Agent, Runner
+ import asyncio
+ 
+ # Create agents
+ agent_a = Agent(name="Agent A", instructions="You are Agent A.")
+ agent_b = Agent(name="Agent B", instructions="You are Agent B.")
+ agent_c = Agent(name="Agent C", instructions="You are Agent C.")
+ 
+ async def main():
+     # Run agents in parallel
+     tasks = [
+         Runner.run(agent_a, "Task for Agent A"),
+         Runner.run(agent_b, "Task for Agent B"),
+         Runner.run(agent_c, "Task for Agent C")
+     ]
+     
+     results = await asyncio.gather(*tasks)
+     
+     # Process results
+     for i, result in enumerate(results):
+         print(f"Agent {chr(65 + i)} result: {result.final_output}")
+ ```
+ 
+ ## Input and Output Guardrails
+ 
+ Apply guardrails for safety and quality control:
+ 
+ ```python
+ from agents import (
+     Agent, 
+     GuardrailFunctionOutput,
+     InputGuardrailTripwireTriggered,
+     OutputGuardrailTripwireTriggered,
+     input_guardrail,
+     output_guardrail,
+     Runner
+ )
+ 
+ # Basic guardrails
+ @input_guardrail
+ async def safety_guardrail(context, agent, input):
+     is_unsafe = "harmful_term" in str(input).lower()
+     return GuardrailFunctionOutput(tripwire_triggered=is_unsafe)
+ 
+ @output_guardrail
+ async def quality_guardrail(context, agent, output, input):
+     is_poor_quality = len(output) < 20
+     return GuardrailFunctionOutput(tripwire_triggered=is_poor_quality)
+ 
+ # Apply guardrails to agent
+ agent = Agent(
+     name="Safe Quality Agent",
+     instructions="You provide helpful, high-quality responses.",
+     input_guardrails=[safety_guardrail],
+     output_guardrails=[quality_guardrail]
+ )
+ ``` 
```

---

Finally, let's create a rule for agent tracing:

---

```diff
- 
+ # OpenAI Agents SDK - Tracing
+ 
+ ## Basic Tracing
+ 
+ The Agents SDK includes built-in tracing to monitor agent activity:
+ 
+ ```python
+ from agents import Agent, Runner, trace
+ import asyncio
+ 
+ async def main():
+     # Trace an entire workflow
+     with trace(workflow_name="My Agent Workflow"):
+         agent = Agent(
+             name="Assistant",
+             instructions="You are a helpful assistant."
+         )
+         
+         result = await Runner.run(agent, "Tell me about tracing in the Agents SDK.")
+         print(result.final_output)
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ```
+ 
+ ## Customizing Traces
+ 
+ Add attributes to traces for better context:
+ 
+ ```python
+ from agents import Agent, Runner, trace
+ import asyncio
+ 
+ async def main():
+     # Add custom attributes to trace
+     with trace(
+         workflow_name="Customer Support",
+         attributes={
+             "customer_id": "12345",
+             "request_type": "technical",
+             "priority": "high"
+         }
+     ):
+         agent = Agent(
+             name="Support Agent",
+             instructions="You help customers with technical issues."
+         )
+         
+         result = await Runner.run(agent, "My computer won't turn on.")
+         print(result.final_output)
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ```
+ 
+ ## Nested Traces
+ 
+ Create hierarchical traces for complex workflows:
+ 
+ ```python
+ from agents import Agent, Runner, trace
+ import asyncio
+ 
+ async def main():
+     # Parent trace for the entire workflow
+     with trace(workflow_name="Multi-Step Workflow"):
+         # Create an agent
+         agent = Agent(
+             name="Assistant",
+             instructions="You are a helpful assistant."
+         )
+         
+         # First subtrace
+         with trace(workflow_name="Step 1 - Information Gathering"):
+             step1_result = await Runner.run(agent, "What are the key points to consider?")
+         
+         # Second subtrace
+         with trace(workflow_name="Step 2 - Analysis"):
+             step2_result = await Runner.run(
+                 agent, 
+                 f"Based on these points: {step1_result.final_output}, what should we do?"
+             )
+         
+         print(step2_result.final_output)
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ```
+ 
+ ## Custom Trace Processors
+ 
+ Create custom processors to send traces to external systems:
+ 
+ ```python
+ from agents import Agent, Runner, trace
+ from agents.tracing import TraceProcessor, TraceEvent
+ import asyncio
+ 
+ class CustomTraceProcessor(TraceProcessor):
+     async def process_trace(self, event: TraceEvent) -> None:
+         # Process the trace event
+         print(f"Event: {event.event_type} for {event.name}")
+         
+         # Here you could send to a monitoring system
+         # await send_to_monitoring_system(event)
+ 
+ # Register your custom processor
+ from agents.tracing import add_trace_processor
+ add_trace_processor(CustomTraceProcessor())
+ 
+ async def main():
+     with trace(workflow_name="Custom Traced Workflow"):
+         agent = Agent(
+             name="Assistant",
+             instructions="You are a helpful assistant."
+         )
+         
+         result = await Runner.run(agent, "Hello world")
+         print(result.final_output)
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ```
+ 
+ ## Run Context for Tracing
+ 
+ Pass context between different operations for connected traces:
+ 
+ ```python
+ from agents import Agent, Runner, RunContext, trace
+ import asyncio
+ 
+ async def main():
+     # Create a context object to track the entire session
+     context = RunContext()
+     
+     with trace(workflow_name="Session Workflow"):
+         agent = Agent(
+             name="Assistant",
+             instructions="You are a helpful assistant."
+         )
+         
+         # First message with context
+         result1 = await Runner.run(
+             agent, 
+             "What is the capital of France?",
+             context=context
+         )
+         
+         # Second message using the same context
+         result2 = await Runner.run(
+             agent, 
+             result1.to_input_list() + [{"role": "user", "content": "What about Italy?"}],
+             context=context
+         )
+         
+         print(result2.final_output)
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ```
+ 
+ ## Debugging with Traces
+ 
+ Use traces for effective debugging:
+ 
+ ```python
+ from agents import Agent, Runner, trace, AgentHooks, RunContextWrapper
+ import asyncio
+ 
+ class DebugHooks(AgentHooks):
+     async def on_start(self, context: RunContextWrapper, agent: Agent) -> None:
+         print(f"â± Starting agent {agent.name}")
+         
+     async def on_end(self, context: RunContextWrapper, agent: Agent, output: str) -> None:
+         print(f"âœ… Finished agent {agent.name}")
+         
+     async def on_error(self, context: RunContextWrapper, agent: Agent, error: Exception) -> None:
+         print(f"âŒ Error in agent {agent.name}: {error}")
+ 
+ async def main():
+     with trace(workflow_name="Debug Workflow"):
+         agent = Agent(
+             name="Debug Agent",
+             instructions="You help debug issues.",
+             hooks=DebugHooks()
+         )
+         
+         try:
+             result = await Runner.run(agent, "Debug this workflow")
+             print(result.final_output)
+         except Exception as e:
+             print(f"Caught exception: {e}")
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ```
+ 
+ ## Integration with External Tracing Systems
+ 
+ Connect with popular observability platforms:
+ 
+ ```python
+ # Example integrating with a popular tracing system
+ from agents import Agent, Runner, trace
+ import asyncio
+ 
+ # This is just an example - check documentation for actual integration code
+ try:
+     # Import specific tracing integration
+     from agents.tracing.integrations import set_up_logfire_integration
+     # Or from external packages
+     # from logfire.integrations.openai_agents import set_up_logfire_integration
+     
+     # Configure the integration
+     set_up_logfire_integration(
+         api_key="your-api-key",
+         dataset="production",
+         service_name="my-agent-service"
+     )
+     
+     print("External tracing configured")
+ except ImportError:
+     print("External tracing not available")
+ 
+ async def main():
+     with trace(workflow_name="Production Workflow"):
+         agent = Agent(
+             name="Production Agent",
+             instructions="You are a production-grade assistant."
+         )
+         
+         result = await Runner.run(agent, "Hello world")
+         print(result.final_output)
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ``` 
```

---

Let's create one more rule for structured output with Pydantic:

---

```diff
- 
+ # OpenAI Agents SDK - Structured Output
+ 
+ ## Basic Structured Output
+ 
+ Define structured output with Pydantic models:
+ 
+ ```python
+ from agents import Agent, Runner
+ from pydantic import BaseModel, Field
+ import asyncio
+ 
+ class MovieReview(BaseModel):
+     title: str
+     year: int
+     rating: float = Field(ge=0, le=10)
+     summary: str
+     strengths: list[str]
+     weaknesses: list[str]
+ 
+ agent = Agent(
+     name="Movie Reviewer",
+     instructions="You review movies and provide structured feedback.",
+     output_type=MovieReview
+ )
+ 
+ async def main():
+     result = await Runner.run(agent, "Review the movie 'The Matrix'")
+     review = result.final_output_as(MovieReview)
+     
+     print(f"Review for {review.title} ({review.year})")
+     print(f"Rating: {review.rating}/10")
+     print(f"Summary: {review.summary}")
+     print("\nStrengths:")
+     for strength in review.strengths:
+         print(f"- {strength}")
+     print("\nWeaknesses:")
+     for weakness in review.weaknesses:
+         print(f"- {weakness}")
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ```
+ 
+ ## Nested Models
+ 
+ Create complex structured outputs with nested models:
+ 
+ ```python
+ from agents import Agent, Runner
+ from pydantic import BaseModel, Field
+ from typing import List
+ import asyncio
+ 
+ class Character(BaseModel):
+     name: str
+     role: str
+     description: str
+ 
+ class PlotPoint(BaseModel):
+     title: str
+     description: str
+     impact: int = Field(ge=1, le=10)
+ 
+ class StoryAnalysis(BaseModel):
+     title: str
+     genre: List[str]
+     main_characters: List[Character]
+     key_plot_points: List[PlotPoint]
+     themes: List[str]
+     recommendation: str
+     rating: float = Field(ge=0, le=10)
+ 
+ agent = Agent(
+     name="Story Analyzer",
+     instructions="Analyze stories and provide detailed structured feedback.",
+     output_type=StoryAnalysis
+ )
+ 
+ async def main():
+     result = await Runner.run(agent, "Analyze the story of 'The Lord of the Rings'")
+     analysis = result.final_output_as(StoryAnalysis)
+     
+     print(f"Analysis of {analysis.title}")
+     print(f"Genre: {', '.join(analysis.genre)}")
+     print(f"Rating: {analysis.rating}/10")
+     
+     print("\nMain Characters:")
+     for character in analysis.main_characters:
+         print(f"- {character.name}: {character.role}")
+         
+     print("\nKey Plot Points:")
+     for plot in analysis.key_plot_points:
+         print(f"- {plot.title} (Impact: {plot.impact}/10)")
+         
+     print(f"\nRecommendation: {analysis.recommendation}")
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ```
+ 
+ ## Working with output_type
+ 
+ The `output_type` parameter tells the agent the expected output structure:
+ 
+ ```python
+ from agents import Agent, Runner
+ from pydantic import BaseModel
+ import asyncio
+ 
+ # Simple output type
+ class WeatherReport(BaseModel):
+     location: str
+     temperature: float
+     conditions: str
+     forecast: str
+ 
+ # Create agent with this output type
+ agent = Agent(
+     name="Weather Reporter",
+     instructions="You provide weather reports for locations.",
+     output_type=WeatherReport
+ )
+ 
+ async def main():
+     result = await Runner.run(agent, "What's the weather in San Francisco?")
+     
+     # Access the result as a WeatherReport object
+     weather = result.final_output_as(WeatherReport)
+     
+     # Work with the structured data
+     print(f"Weather for {weather.location}")
+     print(f"Temperature: {weather.temperature}Â°F")
+     print(f"Conditions: {weather.conditions}")
+     print(f"Forecast: {weather.forecast}")
+     
+     # You can also access the raw output
+     print("\nRaw output:")
+     print(result.final_output)
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ```
+ 
+ ## Field Validation
+ 
+ Use Pydantic's validation for robust outputs:
+ 
+ ```python
+ from agents import Agent, Runner
+ from pydantic import BaseModel, Field, field_validator
+ from typing import List
+ import asyncio
+ 
+ class ProductReview(BaseModel):
+     product_name: str
+     price: float = Field(ge=0)
+     rating: int = Field(ge=1, le=5)
+     pros: List[str] = Field(min_length=1, max_length=5)
+     cons: List[str] = Field(min_length=1, max_length=5)
+     summary: str = Field(min_length=10, max_length=500)
+     
+     @field_validator('pros', 'cons')
+     def check_points_length(cls, value):
+         for item in value:
+             if len(item) < 3:
+                 raise ValueError("Each point must be at least 3 characters")
+         return value
+ 
+ agent = Agent(
+     name="Product Reviewer",
+     instructions="Review products with detailed structured feedback.",
+     output_type=ProductReview
+ )
+ 
+ async def main():
+     result = await Runner.run(agent, "Review an iPhone 13")
+     review = result.final_output_as(ProductReview)
+     
+     print(f"Review for {review.product_name}")
+     print(f"Price: ${review.price}")
+     print(f"Rating: {review.rating}/5")
+     
+     print("\nPros:")
+     for pro in review.pros:
+         print(f"+ {pro}")
+         
+     print("\nCons:")
+     for con in review.cons:
+         print(f"- {con}")
+         
+     print(f"\nSummary: {review.summary}")
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ```
+ 
+ ## Using Enums in Models
+ 
+ Create structured outputs with predefined options:
+ 
+ ```python
+ from agents import Agent, Runner
+ from pydantic import BaseModel, Field
+ from enum import Enum
+ import asyncio
+ 
+ class Sentiment(str, Enum):
+     POSITIVE = "positive"
+     NEUTRAL = "neutral"
+     NEGATIVE = "negative"
+ 
+ class Priority(int, Enum):
+     LOW = 1
+     MEDIUM = 2
+     HIGH = 3
+ 
+ class FeedbackAnalysis(BaseModel):
+     text: str
+     sentiment: Sentiment
+     priority: Priority
+     key_points: list[str] = Field(min_length=1)
+     action_needed: bool
+ 
+ agent = Agent(
+     name="Feedback Analyzer",
+     instructions="Analyze customer feedback and categorize it.",
+     output_type=FeedbackAnalysis
+ )
+ 
+ async def main():
+     result = await Runner.run(
+         agent, 
+         "Customer feedback: The website is hard to navigate and I couldn't find what I needed."
+     )
+     analysis = result.final_output_as(FeedbackAnalysis)
+     
+     print(f"Feedback: {analysis.text}")
+     print(f"Sentiment: {analysis.sentiment.value}")
+     print(f"Priority: {analysis.priority.name} ({analysis.priority.value})")
+     print("\nKey Points:")
+     for point in analysis.key_points:
+         print(f"- {point}")
+     print(f"Action Needed: {'Yes' if analysis.action_needed else 'No'}")
+ 
+ if __name__ == "__main__":
+     asyncio.run(main())
+ ```
+ 
+ ## Multiple Output Types
+ 
+ Use different output types with different agents:
+ 
+ ```python
+ from agents import Agent, Runner
+ from pydantic import BaseModel
+ import asyncio
+ 
+ # Different output types for different agents
+ class QuestionClassification(BaseModel):
+     category: str
+     complexity: int  # 1-5
+     requires_research: bool
+ 
+ class Answer(BaseModel):
+     text: str
+     confidence: float  # 0-1
+     sources: list[str]
+ 
+ # First agent classifies the question
+ classifier_agent = Agent(
+     name="Question Classifier",
+     instructions="Classify questions by category, complexity, and research needs.",
+     output_type=QuestionClassification
+ )
+ 
+ # Second agent answers the question
+ answering_agent = Agent(
+     name="Answering Agent",
+     instructions="Answer questions with high confidence and cite sources.",
+     output_type=Answer
+ )
+ 
+ async def main():
+     user_question = "What causes black holes to form?"
+     
+     # Step 1: Classify the question
+     classification_result = await Runner.run(classifier_agent, user_question)
+     classification = classification_result.final_output_as(QuestionClassification)
+     
+     print(f"Question category: {classification.category}")
+     print(f"Complexity: {classification.complexity}/5")
+     print(f"Requires research: {'Yes' if classification.requires_research else 'No'}")
+     
+     # Step 2: Answer the question
+     answer_prompt = f"""
+     Question: {user_question}
+     Category: {classification.category}
+     Complexity: {classification.complexity}/5
+     
+     Please provide a detailed answer with sources.
+     """
+     
+     answer_result = await Runner.run(answering_agent, answer_prompt)
+     answer = answer_result.final_output_as(Answer)
+     
+     print("\nAnswer:")
+     print(answer.text)
+     print(f"Confidence: {answer.confidence:.2f}")
+     
+     print("\nSources:")
+     for source in answer.sources:
+         print(f"- {source}")
+ 
+ if __name__ == "__main__":
+     asyncio.run(main()) 
```

---

I've created several Cursor .mdc rules based on the OpenAI Agents SDK documentation and examples:

1. `.cursor/rules/openai_agents_basic.mdc` - Basic usage and concepts
2. `.cursor/rules/openai_agents_tools.mdc` - Working with function tools
3. `.cursor/rules/openai_agents_handoffs.mdc` - Agent handoffs and routing
4. `.cursor/rules/openai_agents_guardrails.mdc` - Input and output guardrails
5. `.cursor/rules/openai_agents_patterns.mdc` - Common agent patterns
6. `.cursor/rules/openai_agents_tracing.mdc` - Tracing and debugging
7. `.cursor/rules/openai_agents_structured_output.mdc` - Structured output with Pydantic

Each rule provides:
- Detailed explanations of core concepts
- Code examples for common use cases
- Best practices and patterns
- Implementation details for different scenarios

These rules can be used as references when building applications with the OpenAI Agents SDK. They cover all the major aspects of the SDK, from basic usage to advanced patterns, and provide ready-to-use code snippets that can be adapted for specific use cases.

To use these rules in Cursor, you can access them through the Cursor interface or fetch them with the `fetch_rules` tool. They should be helpful for quickly implementing agent-based systems using the OpenAI Agents SDK.

---

_**User**_

edit them to be set to 'agent requested' rule type and add a description for when to call these agents. here's an example@openai_agents_basic.mdc 

---

_**Assistant**_

I'll edit each of the Cursor rule files to set them as 'agent requested' rule type and add descriptions for when to use each rule. Let me start with each file.

---



---

Let me try again with a different approach:

---

Read file: .cursor/rules/openai_agents_basic.mdc

---

Now I'll edit each file in turn:

---

```diff
  globs: 
  alwaysApply: false
+ type: agent_requested
  ---
  # OpenAI Agents SDK - Basic Usage
+ 
+ ## When to Use This Rule
+ Use this rule when you need to implement basic OpenAI agents functionality. This is appropriate when:
+ - Setting up a simple AI agent that provides text responses
+ - Understanding the fundamental concepts of the Agents SDK
+ - Creating a basic agent with minimal configuration
+ - Learning the basic execution flow for agents
  
  ## Creating and Running Agents
```

---

```diff
  ---
- description: 
+ description: Tools and Function Integrations for OpenAI Agents
  globs: 
  alwaysApply: false
+ type: agent_requested
  ---
  # OpenAI Agents SDK - Tools
+ 
+ ## When to Use This Rule
+ Use this rule when you need to add tools to your OpenAI agents. This is appropriate when:
+ - Creating agents that need to interact with external systems or APIs
+ - Building agents that perform actions beyond just generating text
+ - Implementing function calling capabilities in your agents
+ - Creating custom tools for specific domain capabilities
+ - Tracking tool usage with events and hooks
  
  ## Function Tools
```

---

```diff
  ---
- description: 
+ description: Agent Handoffs and Control Transfer for OpenAI Agents
  globs: 
  alwaysApply: false
+ type: agent_requested
  ---
  # OpenAI Agents SDK - Handoffs
+ 
+ ## When to Use This Rule
+ Use this rule when you need to implement agent handoffs in your application. This is appropriate when:
+ - Building multi-agent systems where different agents handle different tasks
+ - Creating a routing system to direct queries to specialized agents
+ - Implementing language-specific agents that handle different languages
+ - Customizing the conversation history passed between agents
+ - Tracking handoff events between different agents
  
  ## Basic Handoff
```

---

```diff
  ---
- description: 
+ description: Input and Output Guardrails for OpenAI Agents
  globs: 
  alwaysApply: false
+ type: agent_requested
  ---
  # OpenAI Agents SDK - Guardrails
+ 
+ ## When to Use This Rule
+ Use this rule when you need to implement safety and quality guardrails. This is appropriate when:
+ - Building agents that need content moderation and safety controls
+ - Creating systems that validate user inputs before processing
+ - Implementing quality controls for agent responses
+ - Building multi-layer safety systems with multiple validation steps
+ - Handling errors and exceptions from guardrail tripwires
  
  ## Input Guardrails
```

---

```diff
+ ---
+ description: Common Architectural Patterns for OpenAI Agents
+ globs: 
+ alwaysApply: false
+ type: agent_requested
+ ---
  # OpenAI Agents SDK - Common Agent Patterns
+ 
+ ## When to Use This Rule
+ Use this rule when you need to implement advanced agent patterns. This is appropriate when:
+ - Creating multi-step workflows with specialized agents for each step
+ - Building systems that need to route queries to different specialized agents
+ - Implementing agents that leverage other agents as tools
+ - Creating systems that evaluate and refine their own outputs
+ - Running multiple agents in parallel for efficiency or redundancy
+ - Implementing guardrails within complex agent systems
  
  ## Deterministic Flows
```

---

```diff
+ ---
+ description: Tracing and Monitoring for OpenAI Agents
+ globs: 
+ alwaysApply: false
+ type: agent_requested
+ ---
  # OpenAI Agents SDK - Tracing
+ 
+ ## When to Use This Rule
+ Use this rule when you need to implement tracing and monitoring. This is appropriate when:
+ - Debugging complex agent workflows and understanding agent behavior
+ - Monitoring agent performance in production environments
+ - Adding contextual information to trace events for better analytics
+ - Creating hierarchical traces for multi-step or multi-agent workflows
+ - Integrating with external monitoring systems or observability platforms
+ - Building custom trace processors for specialized logging needs
  
  ## Basic Tracing
```

---

```diff
+ ---
+ description: Structured Output and Pydantic Models for OpenAI Agents
+ globs: 
+ alwaysApply: false
+ type: agent_requested
+ ---
  # OpenAI Agents SDK - Structured Output
+ 
+ ## When to Use This Rule
+ Use this rule when you need to implement structured outputs with Pydantic. This is appropriate when:
+ - Creating agents that return structured, validated data instead of free-form text
+ - Building agents that need to return complex, nested data structures
+ - Implementing validation for agent outputs to ensure data quality
+ - Using enums and other constrained types to control agent responses
+ - Coordinating multiple agents that pass structured data between them
+ - Creating systems where output data needs to be processed programmatically
  
  ## Basic Structured Output
```

---

I've updated all seven Cursor rule files to be set as "agent_requested" rule type and added clear descriptions for when to use each rule:

1. **openai_agents_basic.mdc**
   - Type: agent_requested
   - Description: Basic Usage of LLM OpenAI Agents
   - When to use: For simple agents, understanding fundamentals, minimal configuration, basic flows

2. **openai_agents_tools.mdc**
   - Type: agent_requested
   - Description: Tools and Function Integrations for OpenAI Agents
   - When to use: For external system integration, function calling, custom tools, action-oriented agents

3. **openai_agents_handoffs.mdc**
   - Type: agent_requested
   - Description: Agent Handoffs and Control Transfer for OpenAI Agents
   - When to use: For multi-agent systems, routing, specialized agents, conversation history customization

4. **openai_agents_guardrails.mdc**
   - Type: agent_requested
   - Description: Input and Output Guardrails for OpenAI Agents
   - When to use: For content moderation, input validation, quality control, safety systems

5. **openai_agents_patterns.mdc**
   - Type: agent_requested
   - Description: Common Architectural Patterns for OpenAI Agents
   - When to use: For multi-step workflows, routing systems, agent-as-tools, self-evaluation, parallel execution

6. **openai_agents_tracing.mdc**
   - Type: agent_requested
   - Description: Tracing and Monitoring for OpenAI Agents
   - When to use: For debugging, monitoring, analytics, hierarchical traces, external integrations

7. **openai_agents_structured_output.mdc**
   - Type: agent_requested
   - Description: Structured Output and Pydantic Models for OpenAI Agents
   - When to use: For structured data returns, complex data structures, validation, constrained responses

Each rule now clearly indicates when it should be used, making it easier for developers to select the appropriate rule for their specific needs.

---

_**User**_

rename all of them to not include '_' but '-'

---

_**Assistant**_

I'll rename all the Cursor rule files to replace underscores (`_`) with hyphens (`-`). Let me do that for each file:

---

```bash
mv .cursor/rules/openai_agents_basic.mdc .cursor/rules/openai-agents-basic.mdc
```

```
(env) (base) itaicohen@MacBook-Pro openai-agents-python % 

```

---

```bash
mv .cursor/rules/openai_agents_tools.mdc .cursor/rules/openai-agents-tools.mdc
```

```
(env) (base) itaicohen@MacBook-Pro openai-agents-python % 

```

---

```bash
mv .cursor/rules/openai_agents_handoffs.mdc .cursor/rules/openai-agents-handoffs.mdc
```

```
(env) (base) itaicohen@MacBook-Pro openai-agents-python % 

```

---

```bash
mv .cursor/rules/openai_agents_guardrails.mdc .cursor/rules/openai-agents-guardrails.mdc
```

```
(env) (base) itaicohen@MacBook-Pro openai-agents-python % 

```

---

```bash
mv .cursor/rules/openai_agents_patterns.mdc .cursor/rules/openai-agents-patterns.mdc
```

```
(env) (base) itaicohen@MacBook-Pro openai-agents-python % 

```

---

```bash
mv .cursor/rules/openai_agents_tracing.mdc .cursor/rules/openai-agents-tracing.mdc
```

```
(env) (base) itaicohen@MacBook-Pro openai-agents-python % 

```

---

```bash
mv .cursor/rules/openai_agents_structured_output.mdc .cursor/rules/openai-agents-structured-output.mdc
```

```
(env) (base) itaicohen@MacBook-Pro openai-agents-python % 

```

---

```bash
ls -la .cursor/rules/
```

```
(env) (base) itaicohen@MacBook-Pro openai-agents-python % ls -la .cursor/rules/
total 112
drwxr-xr-x@ 10 itaicohen  staff   320 Mar 18 12:45 .
drwxr-xr-x@  3 itaicohen  staff    96 Mar 18 12:34 ..
-rw-r--r--@  1 itaicohen  staff  1038 Mar 18 12:35 new-project.mdc
-rw-r--r--@  1 itaicohen  staff  2205 Mar 18 12:43 openai-agents-basic.mdc
-rw-r--r--@  1 itaicohen  staff  5217 Mar 18 12:43 openai-agents-guardrails.mdc
-rw-r--r--@  1 itaicohen  staff  4272 Mar 18 12:43 openai-agents-handoffs.mdc
-rw-r--r--@  1 itaicohen  staff  7300 Mar 18 12:43 openai-agents-patterns.mdc
-rw-r--r--@  1 itaicohen  staff  8941 Mar 18 12:44 openai-agents-structured-output.mdc
-rw-r--r--@  1 itaicohen  staff  3454 Mar 18 12:43 openai-agents-tools.mdc
-rw-r--r--@  1 itaicohen  staff  7135 Mar 18 12:44 openai-agents-tracing.mdc
(env) (base) itaicohen@MacBook-Pro openai-agents-python %
```

---

All the rule files have been successfully renamed. I've changed all instances of underscores (`_`) to hyphens (`-`) in the filenames:

1. `openai_agents_basic.mdc` â†’ `openai-agents-basic.mdc`
2. `openai_agents_tools.mdc` â†’ `openai-agents-tools.mdc`
3. `openai_agents_handoffs.mdc` â†’ `openai-agents-handoffs.mdc`
4. `openai_agents_guardrails.mdc` â†’ `openai-agents-guardrails.mdc`
5. `openai_agents_patterns.mdc` â†’ `openai-agents-patterns.mdc`
6. `openai_agents_tracing.mdc` â†’ `openai-agents-tracing.mdc`
7. `openai_agents_structured_output.mdc` â†’ `openai-agents-structured-output.mdc`

The `ls -la` command confirms that all files have been renamed correctly and are available in the `.cursor/rules/` directory.

---

_**User**_

let's set this project for git and create a new repo

---

_**Assistant**_



---



---

